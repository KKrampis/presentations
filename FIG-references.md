Here are your references converted to markdown links:

1. [[1] Refusal in LLMs is Mediated by a Single Direction](https://www.alignmentforum.org/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction)
2. [[2] Colab Notebook](https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw)
3. [[3] Trusted Monitoring But With Deception Probes](https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes)
4. [[4] Detecting Strategic Deception Using Linear Probes](https://www.alignmentforum.org/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes)
5. [[5] A Problem to Solve Before Building a Deception Detector](https://www.alignmentforum.org/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector)
6. [[6] Intentional Stance - Wikipedia](https://en.wikipedia.org/wiki/Intentional_stance)
7. [[7] LLM Attacks - Harmful Behaviors Dataset](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv) (harmful prompts)
8. [[8] Jailbreak LLM - Malicious Instruct](https://github.com/Princeton-SysML/Jailbreak_LLM/blob/main/data/MaliciousInstruct.txt) (harmful prompts)
9. [[9] TDC 2023 Red Teaming Data](https://github.com/centerforaisafety/tdc2023-starter-kit/tree/main/red_teaming/data) (harmful prompts)
10. [[10] Alpaca Dataset - Hugging Face](https://huggingface.co/datasets/tatsu-lab/alpaca) (harmless prompts)
11. [[11] Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
12. [[12] Towards Monosemanticity](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
